#!/usr/bin/env groovy

@Library('apm@current') _

pipeline {
  agent { label 'ubuntu-20 && immutable' }
  environment {
    REPO="package-storage"
    BASE_DIR="src/github.com/elastic/package-storage"
    DOCKER_REGISTRY = 'docker.elastic.co'
    DOCKER_REGISTRY_SECRET = 'secret/observability-team/ci/docker-registry/prod'
    PIPELINE_LOG_LEVEL='INFO'
    DOCKER_IMG = "${env.DOCKER_REGISTRY}/package-registry/distribution"
    DOCKER_IMG_PR = "${env.DOCKER_REGISTRY}/observability-ci/package-registry/distribution"

    JOB_SIGNING_CREDENTIALS = 'sign-artifacts-with-gpg-job'
    INTERNAL_CI_JOB_GCS_CREDENTIALS = 'internal-ci-gcs-plugin'
    INFRA_SIGNING_BUCKET_NAME = 'internal-ci-artifacts'
    REPO_BUILD_TAG = "${env.REPO}/${env.BUILD_TAG}"
    INFRA_SIGNING_BUCKET_ARTIFACTS_PATH = "gs://${env.INFRA_SIGNING_BUCKET_NAME}/${env.REPO_BUILD_TAG}"
    INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_PATH = "gs://${env.INFRA_SIGNING_BUCKET_NAME}/${env.INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_SUBFOLDER}"
    INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_SUBFOLDER = "${env.REPO_BUILD_TAG}/signed-artifacts"

    PACKAGE_STORAGE_UPLOADER_CREDENTIALS = 'upload-package-to-package-storage'
    PACKAGE_STORAGE_UPLOADER_GCP_SERVICE_ACCOUNT = 'secret/gce/elastic-bekitzur/service-account/package-storage-uploader'
    PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH = "gs://elastic-bekitzur-package-storage-internal/queue-publishing/${env.REPO_BUILD_TAG}"
    BUILD_ARTIFACTS_PATH = 'build/packages'
  }
  options {
    timeout(time: 4, unit: 'HOURS')
    buildDiscarder(logRotator(numToKeepStr: '20', artifactNumToKeepStr: '20', daysToKeepStr: '30'))
    timestamps()
    ansiColor('xterm')
    disableResume()
    durabilityHint('PERFORMANCE_OPTIMIZED')
    rateLimitBuilds(throttle: [count: 60, durationName: 'hour', userBoost: true])
    quietPeriod(10)
  }
  triggers {
    issueCommentTrigger('(?i)(.*(?:jenkins\\W+)?run\\W+(?:the\\W+)?tests(?:\\W+please)?.*|^\\/test$)')
  }
  parameters {
    booleanParam(name: 'run_all_stages', defaultValue: false, description: 'Force to run all stages.')
  }
  stages {
    /**
     Checkout the code and stash it, to use it on other stages.
     */
    stage('Checkout') {
      steps {
        deleteDir()
        gitCheckout(basedir: "${BASE_DIR}")
        setEnvVar("GO_VERSION", readFile(file: "${BASE_DIR}/.go-version")?.trim())
        stash allowEmpty: true, name: 'source', useDefaultExcludes: false
      }
    }
    /**
     Checks formatting / linting.
     */
    stage('Lint') {
      steps {
        cleanup()
        dir("${BASE_DIR}"){
          whenTrue(isGitRegionMatch(patterns: [".*/_.*"])) {
            error('_dev in packages are intended to exist only for development purposes.')
          }
        }
        withMageEnv(){
          dir("${BASE_DIR}"){
            sh(label: 'Checks formatting / linting',script: 'mage -debug check')
          }
        }
      }
    }
    /**
     Builds and validates the packages
     */
    stage('Build') {
      steps {
        cleanup()
        withMageEnv(){
          dir("${BASE_DIR}"){
            sh(label: 'Checks if the packages can be build and are valid',script: 'mage -debug build')
          }
        }
      }
    }
    /**
     Execute integration tests.
     */
    /*stage('TestIntegration') {
      environment {
        HOME = "${env.WORKSPACE}"
      }
      steps {
        cleanup()
        withMageEnv(){
          dir("${BASE_DIR}"){
            sh(label: 'Runs the (integration) tests',script: 'mage -debug testIntegration|tee tests-report.txt')
          }
        }
      }
      post {
        always {
          archiveArtifacts(allowEmptyArchive: true, artifacts: "${BASE_DIR}/build/elastic-stack-logs/*.log")
          convertGoTestResults(
            input: "${BASE_DIR}/tests-report.txt",
            output: "${BASE_DIR}/junit-report.xml"
          )
        }
      }
    }*/
    stage('Zip and sign') {
      /*when {
        branch 'production'
      }*/
      steps {
        script {
          cleanup()
          dir("${BASE_DIR}") {
            def unpublishedPresent = buildUnpublishedPackages()
            if (!unpublishedPresent) {
              echo 'All packages are in sync'
              return
            }
            signArtifactsWithElastic()
            //publishToPackageStorage()
          }
        }
      }
    }
    /**
     Publish PR Docker images.
     */
    stage('Publish PR Docker image'){
      when {
        changeRequest(target: '(snapshot|staging|production)', comparator: 'REGEXP')
      }
      environment {
        DOCKER_IMG_TAG = "${env.DOCKER_IMG_PR}:${env.GIT_BASE_COMMIT}"
        DOCKER_IMG_TAG_BRANCH = "${env.DOCKER_IMG_PR}:${env.BRANCH_NAME}"
      }
      steps {
        //cleanup()
        pushDockerImage()
      }
    }
  }
  post {
    cleanup {
      notifyBuildResult(prComment: true)
    }
  }
}

def cleanup(){
  dir("${BASE_DIR}"){
    deleteDir()
  }
  unstash 'source'
}


def pushDockerImage(){
  dir("${BASE_DIR}"){
    dockerLogin(secret: "${env.DOCKER_REGISTRY_SECRET}",
      registry: "${env.DOCKER_REGISTRY}")
    sh(label: 'Build Docker image',
      script: """docker build \
        -t ${env.DOCKER_IMG_TAG} \
        --label BRANCH_NAME=${env.BRANCH_NAME} \
        --label GIT_SHA=${env.GIT_BASE_COMMIT} \
        --label GO_VERSION=${env.GO_VERSION} \
        --label TIMESTAMP=\$(date +%Y-%m-%d_%H:%M) \
        .
    """)
    retryWithSleep(retries: 3, seconds: 5, backoff: true){
      sh(label: 'Push Docker image sha',
        script: "docker push ${env.DOCKER_IMG_TAG}")
      sh(label: 'Re-tag Docker image',
        script: "docker tag ${env.DOCKER_IMG_TAG} ${env.DOCKER_IMG_TAG_BRANCH}")
      sh(label: 'Push Docker image name',
        script: "docker push ${env.DOCKER_IMG_TAG_BRANCH}")
    }
  }
}

def buildUnpublishedPackages() {
  def unpublishedPresent = false
  dir("packages") {
    def packageRevision = [:]
    findFiles()?.findAll{ !it.name.endsWith('@tmp') }?.collect{ it.name }?.sort()?.each {
      def packageName = it
      dir("${packageName}") {
        findFiles()?.findAll{ !it.name.endsWith('@tmp') }?.collect{ it.name }?.sort()?.each {
          def packageVersion = it
          // Check if the package revision is published
          /*def responseCode = httpRequest(method: "HEAD",
            url: "https://package-storage.elastic.co/artifacts/packages/${packageName}-${packageVersion}.zip",
            response_code_only: true)
          if (responseCode == 200) {
            return // already published
          }*/
          sh(label: "Pre-compression actions", script: "mkdir -p target && cp -r ${packageVersion} target/${packageName}-${packageVersion}")
          zip(zipFile: "${packageName}-${packageVersion}.zip", dir: "target")
          sh(label: "Post-compression actions", script: """
            mkdir -p ../../${env.BUILD_ARTIFACTS_PATH}
            mv ${packageName}-${packageVersion}.zip ../../${env.BUILD_ARTIFACTS_PATH}/
            rm -r target
            """)
          unpublishedPresent = true
        }
      }
    }
  }
  return unpublishedPresent
}

def signArtifactsWithElastic() {
  googleStorageUpload(bucket: env.INFRA_SIGNING_BUCKET_ARTIFACTS_PATH,
    credentialsId: env.INTERNAL_CI_JOB_GCS_CREDENTIALS,
    pathPrefix: env.BUILD_ARTIFACTS_PATH + '/',
    pattern: env.BUILD_ARTIFACTS_PATH + '/*.zip',
    sharedPublicly: false,
    showInline: true)
  withCredentials([string(credentialsId: env.JOB_SIGNING_CREDENTIALS, variable: 'TOKEN')]) {
    triggerRemoteJob(auth: CredentialsAuth(credentials: 'local-readonly-api-token'),
      job: 'https://internal-ci.elastic.co/job/elastic+unified-release+master+sign-artifacts-with-gpg',
      token: TOKEN,
      parameters: "gcs_input_path=${env.INFRA_SIGNING_BUCKET_ARTIFACTS_PATH}",
      useCrumbCache: true,
      useJobInfoCache: true)
  }
  googleStorageDownload(bucketUri: "${env.INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_PATH}/*",
    credentialsId: env.INTERNAL_CI_JOB_GCS_CREDENTIALS,
    localDirectory: env.BUILD_ARTIFACTS_PATH + '/',
    pathPrefix: "${env.INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_SUBFOLDER}")
    sh(label: 'Rename .asc to .sig', script: 'for f in ' + env.BUILD_ARTIFACTS_PATH + '/*.asc; do mv "$f" "${f%.asc}.sig"; done')
}

def publishToPackageStorage() {
  dir(env.BUILD_ARTIFACTS_PATH) {
    withGCPEnv(secret: env.PACKAGE_STORAGE_UPLOADER_GCP_SERVICE_ACCOUNT) {
      withCredentials([string(credentialsId: env.PACKAGE_STORAGE_UPLOADER_CREDENTIALS, variable: 'TOKEN')]) {
        findFiles()?.findAll{ it.name.endsWith('.zip') }?.collect{ it.name }?.sort()?.each {
          def packageZip = it
          sh(label: 'Upload package .zip file', script: "gsutil cp ${packageZip} ${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/")
          sh(label: 'Upload package .sig file', script: "gsutil cp ${packageZip}.sig ${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/")

          triggerRemoteJob(auth: CredentialsAuth(credentials: 'local-readonly-api-token'),
            job: 'https://internal-ci.elastic.co/job/package_storage/job/publishing-job-remote',
            token: TOKEN,
            parameters: """
              dry_run=false
              gs_package_build_zip_path=${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/${packageZip}
              gs_package_signature_path=${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/${packageZip}.sig
              legacy_package=true
              """,
              useCrumbCache: true,
              useJobInfoCache: true)
        }
      }
    }
  }
}