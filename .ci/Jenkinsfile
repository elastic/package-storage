#!/usr/bin/env groovy

@Library('apm@current') _

pipeline {
  agent { label 'ubuntu-20 && immutable' }
  environment {
    REPO="package-storage"
    BASE_DIR="src/github.com/elastic/package-storage"
    DOCKER_REGISTRY = 'docker.elastic.co'
    DOCKER_REGISTRY_SECRET = 'secret/observability-team/ci/docker-registry/prod'
    PIPELINE_LOG_LEVEL='INFO'
    DOCKER_IMG = "${env.DOCKER_REGISTRY}/package-registry/distribution"
    DOCKER_IMG_PR = "${env.DOCKER_REGISTRY}/observability-ci/package-registry/distribution"

    JOB_SIGNING_CREDENTIALS = 'sign-artifacts-with-gpg-job'
    INTERNAL_CI_JOB_GCS_CREDENTIALS = 'internal-ci-gcs-plugin'
    INFRA_SIGNING_BUCKET_NAME = 'internal-ci-artifacts'
    REPO_BUILD_TAG = "${env.REPO}/${env.BUILD_TAG}"
    INFRA_SIGNING_BUCKET_ARTIFACTS_PATH = "gs://${env.INFRA_SIGNING_BUCKET_NAME}/${env.REPO_BUILD_TAG}"
    INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_PATH = "gs://${env.INFRA_SIGNING_BUCKET_NAME}/${env.INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_SUBFOLDER}"
    INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_SUBFOLDER = "${env.REPO_BUILD_TAG}/signed-artifacts"

    PACKAGE_STORAGE_UPLOADER_CREDENTIALS = 'upload-package-to-package-storage'
    PACKAGE_STORAGE_UPLOADER_GCP_SERVICE_ACCOUNT = 'secret/gce/elastic-bekitzur/service-account/package-storage-uploader'
    PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH = "gs://elastic-bekitzur-package-storage-internal/queue-publishing/${env.REPO_BUILD_TAG}"
  }
  options {
    timeout(time: 12, unit: 'HOURS')
    buildDiscarder(logRotator(numToKeepStr: '20', artifactNumToKeepStr: '20', daysToKeepStr: '30'))
    timestamps()
    ansiColor('xterm')
    disableResume()
    durabilityHint('PERFORMANCE_OPTIMIZED')
    rateLimitBuilds(throttle: [count: 60, durationName: 'hour', userBoost: true])
    quietPeriod(10)
  }
  triggers {
    issueCommentTrigger('(?i)(.*(?:jenkins\\W+)?run\\W+(?:the\\W+)?tests(?:\\W+please)?.*|^\\/test$)')
  }
  parameters {
    booleanParam(name: 'run_all_stages', defaultValue: false, description: 'Force to run all stages.')
  }
  stages {
    /**
     Checkout the code and stash it, to use it on other stages.
     */
    stage('Checkout') {
      steps {
        deleteDir()
        gitCheckout(basedir: "${BASE_DIR}")
        stash allowEmpty: true, name: 'source', useDefaultExcludes: false
      }
    }
    /**
     Checks formatting / linting.
     */
    stage('Lint') {
      steps {
        cleanup()
        dir("${BASE_DIR}"){
          whenTrue(isGitRegionMatch(patterns: [".*/_.*"])) {
            error('_dev in packages are intended to exist only for development purposes.')
          }
        }
        withMageEnv(){
          dir("${BASE_DIR}"){
            sh(label: 'Checks formatting / linting',script: 'mage -debug check')
          }
        }
      }
    }
    /**
     Builds and validates the packages
     */
    stage('Build') {
      steps {
        cleanup()
        withMageEnv(){
          dir("${BASE_DIR}"){
            sh(label: 'Checks if the packages can be build and are valid',script: 'mage -debug build')
          }
        }
      }
    }
    /**
     Execute integration tests.
     */
    // Disable it temporarily
    /*stage('TestIntegration') {
      environment {
        HOME = "${env.WORKSPACE}"
      }
      steps {
        cleanup()
        withMageEnv(){
          dir("${BASE_DIR}"){
            sh(label: 'Runs the (integration) tests',script: 'mage -debug testIntegration|tee tests-report.txt')
          }
        }
      }
      post {
        always {
          archiveArtifacts(allowEmptyArchive: true, artifacts: "${BASE_DIR}/build/elastic-stack-logs/*.log")
          convertGoTestResults(
            input: "${BASE_DIR}/tests-report.txt",
            output: "${BASE_DIR}/junit-report.xml"
          )
        }
      }
    }
    /**
     Publish Docker images.
     */
    /*stage('Publish Docker image'){
      when {
        anyOf {
          branch 'snapshot'
          branch 'staging'
          branch 'production'
        }
      }
      environment {
        DOCKER_IMG_TAG = "${env.DOCKER_IMG}:${env.GIT_BASE_COMMIT}"
        DOCKER_IMG_TAG_BRANCH = "${env.DOCKER_IMG}:${env.BRANCH_NAME}"
      }
      steps {
        cleanup()
        pushDockerImage()
      }
    }
    /**
     Publish PR Docker images.
     */
    /*stage('Publish PR Docker image'){
      when {
        changeRequest(target: '(snapshot|staging|production)', comparator: 'REGEXP')
      }
      environment {
        DOCKER_IMG_TAG = "${env.DOCKER_IMG_PR}:${env.GIT_BASE_COMMIT}"
        DOCKER_IMG_TAG_BRANCH = "${env.DOCKER_IMG_PR}:${env.BRANCH_NAME}"
      }
      steps {
        cleanup()
        pushDockerImage()
      }
    }*/
    stage('v2: Sync packages') {
      steps {
        script {
          cleanup()
          dir("${BASE_DIR}") {
            // Comprise a list of unpublished packages
            dir("packages") {
              def packageRevision = [:]
              findFiles()?.findAll{ !it.name.endsWith('@tmp') }?.collect{ it.name }?.sort()?.each {
                def packageName = it
                dir("${packageName}") {
                  findFiles()?.findAll{ !it.name.endsWith('@tmp') }?.collect{ it.name }?.sort()?.each {
                    def packageVersion = it
                    // Check if the package revision is published
                    def responseCode = httpRequest(method: "HEAD",
                      url: "https://package-storage.elastic.co/artifacts/packages/${packageName}-${packageVersion}.zip",
                      response_code_only: true)
                    if (responseCode == 200) {
                      continue // already published
                    }
                    sh(label: "Pre-compression actions", script: "mkdir -p target && cp -r ${packageVersion} target/${packageName}-${packageVersion}")
                    zip(zipFile: "${packageName}-${packageVersion}.zip", dir: "target")
                    sh(label: "Post-compression actions", script: """
                    mkdir -p ../../build/packages
                    mv ${packageName}-${packageVersion}.zip ../../build/packages/
                    rm -r target
                    """)
                  }
                }
              }
            }

            // Sign unpublished packages
            signArtifactsWithElastic("build/packages", "build/packages-elastic-signatures")

            // Push those packages to Package Storage v2
            dir("build/packages-elastic-signatures") {
              withGCPEnv(secret: env.PACKAGE_STORAGE_UPLOADER_GCP_SERVICE_ACCOUNT) {
                withCredentials([string(credentialsId: env.PACKAGE_STORAGE_UPLOADER_CREDENTIALS, variable: 'TOKEN')]) {
                  findFiles()?.findAll{ it.name.endsWith('.zip') }?.collect{ it.name }?.sort()?.each {
                    def packageZip = it
                    sh(label: 'Upload package .zip file', script: "gsutil cp ${packageZip} ${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/")
                    sh(label: 'Upload package .sig file', script: "gsutil cp ${packageZip}.sig ${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/")

                    triggerRemoteJob(auth: CredentialsAuth(credentials: 'local-readonly-api-token'),
                      job: 'https://internal-ci.elastic.co/job/package_storage/job/publishing-job-remote',
                      token: TOKEN,
                      parameters: """
                        dry_run=true
                        gs_package_build_zip_path=${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/package_storage_candidate-0.0.1.zip
                        gs_package_signature_path=${env.PACKAGE_STORAGE_INTERNAL_BUCKET_QUEUE_PUBLISHING_PATH}/package_storage_candidate-0.0.1.zip.sig
                        """,
                      useCrumbCache: true,
                      useJobInfoCache: true)
                  }
                }
              }
            }
          }
        }
      }
    }
  }
  post {
    cleanup {
      notifyBuildResult(prComment: true)
    }
  }
}

def cleanup(){
  dir("${BASE_DIR}"){
    deleteDir()
  }
  unstash 'source'
}


def pushDockerImage(){
  dir("${BASE_DIR}"){
    dockerLogin(secret: "${env.DOCKER_REGISTRY_SECRET}",
      registry: "${env.DOCKER_REGISTRY}")
    sh(label: 'Build Docker image',
      script: """docker build \
        -t ${env.DOCKER_IMG_TAG} \
        --label BRANCH_NAME=${env.BRANCH_NAME} \
        --label GIT_SHA=${env.GIT_BASE_COMMIT} \
        --label GO_VERSION=${env.GO_VERSION} \
        --label TIMESTAMP=\$(date +%Y-%m-%d_%H:%M) \
        .
    """)
    retryWithSleep(retries: 3, seconds: 5, backoff: true){
      sh(label: 'Push Docker image sha',
        script: "docker push ${env.DOCKER_IMG_TAG}")
      sh(label: 'Re-tag Docker image',
        script: "docker tag ${env.DOCKER_IMG_TAG} ${env.DOCKER_IMG_TAG_BRANCH}")
      sh(label: 'Push Docker image name',
        script: "docker push ${env.DOCKER_IMG_TAG_BRANCH}")
    }
  }
}

def signArtifactsWithElastic(artifactsSourcePath, signaturesDestinationPath) {
  googleStorageUpload(bucket: env.INFRA_SIGNING_BUCKET_ARTIFACTS_PATH,
    credentialsId: env.INTERNAL_CI_JOB_GCS_CREDENTIALS,
    pathPrefix: artifactsSourcePath + '/',
    pattern: artifactsSourcePath + '/*.zip',
    sharedPublicly: false,
    showInline: true)
  withCredentials([string(credentialsId: env.JOB_SIGNING_CREDENTIALS, variable: 'TOKEN')]) {
    triggerRemoteJob(auth: CredentialsAuth(credentials: 'local-readonly-api-token'),
      job: 'https://internal-ci.elastic.co/job/elastic+unified-release+master+sign-artifacts-with-gpg',
      token: TOKEN,
      parameters: "gcs_input_path=${env.INFRA_SIGNING_BUCKET_ARTIFACTS_PATH}",
      useCrumbCache: true,
      useJobInfoCache: true)
  }
  googleStorageDownload(bucketUri: "${env.INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_PATH}/*",
    credentialsId: env.INTERNAL_CI_JOB_GCS_CREDENTIALS,
    localDirectory: signaturesDestinationPath + '/',
    pathPrefix: "${env.INFRA_SIGNING_BUCKET_SIGNED_ARTIFACTS_SUBFOLDER}")
    sh(label: 'Rename .asc to .sig', script: 'for f in ' + signaturesDestinationPath + '/*.asc; do mv "$f" "${f%.asc}.sig"; done')
}